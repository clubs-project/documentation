@article{chatzitheodorou_costa_2013,
	title = {{COSTA} {MT} {Evaluation} {Tool}: {An} {Open} {Toolkit} for {Human} {Machine} {Translation} {Evaluation}},
	volume = {100},
	journal = {The Prague Bulletin of Mathematical Linguistics},
	author = {Chatzitheodorou, Konstantinos},
	year = {2013},
	pages = {83--89}
}

@inproceedings{vela_human_2014,
	title = {Human {Translation} {Evaluation} and its {Coverage} by {Automatic} {Scores}},
	booktitle = {Automatic and {Manual} {Metrics} for {Operational} {Translation} {Evaluation} {Workshop} {Programme}},
	author = {Vela, Mihaela and Schumann, Anne-Kathrin and Wurm, Andrea},
	year = {2014},
	pages = {19}
}

@inproceedings{girardi_mt-equal:_2014,
	title = {{MT}-{EQuAl}: a {Toolkit} for {Human} {Assessment} of {Machine} {Translation} {Output}.},
	booktitle = {{COLING} ({Demos})},
	author = {Girardi, Christian and Bentivogli, Luisa and Farajian, Mohammad Amin and Federico, Marcello},
	year = {2014},
	pages = {120--123}
}

@article{mendoza_mompean_human_2013,
	title = {Human evaluation of {MT}},
	author = {Mendoza Mompeán, Ginés and {others}},
	year = {2013}
}

@article{temnikova_how_2015,
	title = {How do {Humans} {Evaluate} {Machine} {Translation}},
	journal = {EMNLP 2015},
	author = {Temnikova, Francisco Guzmán Ahmed Abdelali Irina and Sajjad, Hassan and Vogel, Stephan},
	year = {2015},
	pages = {457}
}

@incollection{dorr_machine_2011,
	address = {New York, NY},
	title = {Machine {Translation} {Evaluation} and {Optimization}},
	isbn = {978-1-4419-7713-7},
	url = {http://dx.doi.org/10.1007/978-1-4419-7713-7_5},
	booktitle = {Handbook of {Natural} {Language} {Processing} and {Machine} {Translation}: {DARPA} {Global} {Autonomous} {Language} {Exploitation}},
	publisher = {Springer New York},
	author = {Dorr, Bonnie and Olive, Joseph and McCary, John and Christianson, Caitlin},
	editor = {Olive, Joseph and Christianson, Caitlin and McCary, John},
	year = {2011},
	pages = {745--843}
}


@inproceedings{papineni_bleu:_2002,
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	booktitle = {Proceedings of the 40th annual meeting on association for computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	pages = {311--318}
}


Human evaluation of machine translation through binary system comparisons
Full Text:	PDFPDF
Authors:	David Vilar	RWTH Aachen University, Aachen, Germany
Gregor Leusch	RWTH Aachen University, Aachen, Germany
Hermann Ney	RWTH Aachen University, Aachen, Germany
Rafael E. Banchs	Universitat Politècnica de Catalunya, Barcelona, Spain
Human evaluation of machine translation through binary system comparisons	2007 Article
Bibliometrics Data  Bibliometrics
· Citation Count: 5
· Downloads (cumulative): 130
· Downloads (12 Months): 33
· Downloads (6 Weeks): 3

Published in:
Cover Image
 
· Proceeding
StatMT '07 Proceedings of the Second Workshop on Statistical Machine Translation
Pages 96-103 

Prague, Czech Republic — June 23 - 23, 2007 
Association for Computational Linguistics Stroudsburg, PA, USA ©2007 
table of contents

Author Silhouette    Recent authors with related interests  Expand Related Authors    Concepts in this article  Expand Paper Concepts
About ACM and IBM Watsonpowered by
Tools and Resources
Save to Binder
Export Formats:
BibTeX EndNote ACM Ref
Share:
|
 Contact The DL Team Contact Us | Switch to single page view (no tabs)
Abstract
Authors
References
Cited By
Index Terms
Publication
Reviews
Comments
Table of Contents
We introduce a novel evaluation scheme for the human evaluation of different machine translation systems. Our method is based on direct comparison of two sentences at a time by human judges. These binary judgments are then used to decide between all possible rankings of the systems. The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems. Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation. We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates. We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign.

Powered by The ACM Guide to Computing Literature

@inproceedings{Vilar:2007,
 author = {Vilar, David and Leusch, Gregor and Ney, Hermann and Banchs, Rafael E.},
 title = {Human Evaluation of Machine Translation Through Binary System Comparisons},
 booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
 series = {StatMT '07},
 year = {2007},
 location = {Prague, Czech Republic},
 pages = {96--103},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1626355.1626368},
 acmid = {1626368},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@book{chen_multilingual_2016,
	address = {Santa Barbara, California, USA},
	title = {Multilingual {Access} and {Services} for {Digital} {Collections}},
	publisher = {Libraries Unlimited},
	author = {Chen, Jiangping},
	year = {2016}
}