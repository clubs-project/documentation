\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{booktabs, array, pdflscape}
\usepackage{geometry}
\usepackage{graphics,subfigure,graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}

\setlength{\textheight}{24cm}  
\setlength{\textwidth}{15cm}
\setlength\oddsidemargin{0cm}
\setlength\evensidemargin{0cm}
\setlength\voffset{-1cm}

\renewcommand{\textfraction}{0.01}
\renewcommand{\floatpagefraction}{0.75}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}

\newcommand{\red}[1]{\textcolor{red}{#1}}	
\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}

\newcommand{\Ni}{({\em i\,})~}
\newcommand{\Nii}{({\em ii\,})~}
\newcommand{\Niii}{({\em iii\,})~}

%opening
\title{
\includegraphics[width=3cm]{./img/200px-SuitClubs.png} \\
\Huge M1.4 -- MT Approaches towards Cross-lingual IR in Pubpsych}
\author{\vspace*{1cm}\\ \LARGE Cristina Espa\~na-Bonet \medskip \\ \Large Universit\"at des Saarlandes}
\date{\vspace*{2cm} -- v1.0 --\\August 2018}



\begin{document}

\clearpage\maketitle
\thispagestyle{empty}\thispagestyle{empty}

\vspace*{5cm}
\begin{abstract}
This document describes the architecture options and final choices for implementing the machine translation (MT) system aimed to translate articles' titles and abstracts. The alternatives are presented and a comparison among the two most promising architectures, Statistical MT and Neural MT, is given. 
\end{abstract}

\newpage
\tableofcontents
\clearpage

\section{Introduction}
\label{s:intro}

The main purpose of the project is to provide PubPsych with a Cross Language Information Retrieval (CLIR) functionality to allow users to specify their information needs in their preferred language while retrieving relevant documents matching their needs in languages different from the query language.

One of the possible approaches to CLIR is Machine Translation (MT). The translation can take place at different points of the retrieval process (??):
\begin{itemize}
\itemsep0em 
 \item[\Ni] Translating the query: When a search expression is entered, the portal extends the query by injecting translations of the terms or phrases found in it, and runs the modified version against the search index
 \item[\Nii] Translating the metadata: Before indexing, some or all of the content (e.g. keywords, titles \& abstracts) of the portal is translated, so that all query languages are represented in the index
\end{itemize}

PubPsych offers multilingual content in English, French, German and Spanish. All its metadata is in one or more of the four languages. The original works, such as articles, book chapters, data sets, might be in many different languages (more than 50 in PubPsych), but the metadata are always available in at least either English, French, German or Spanish. So, both in Scenario \Ni and \Nii the translation needs to be done among the four languages. Again, there are two possibilities:
\begin{itemize}
\itemsep0em 
 \item[\Ni] Training in the 12 translation directions to be able to perform any combination,
 \item[\Nii] Using English as a pivot language
\end{itemize}

Choosing the best option depends on the nature of the translation system. To give an example, an MT system for translating the abstracts of our documents can be built either with statistical or neural architectures. Statistical systems would offer a higher quality when using English as pivot, as we do not have parallel corpora for the six language pairs and the translation from German-to-Spanish would be done via English anyway. Besides, the number of systems to maintain diminishes from 12 to 3.
On the other hand, neural systems are able to learn to translate among all the languages even when there is no direct parallel data for a language pair (with a lower quality though). In this case, a single multilingual neural system is enough to translate among the 4 languages. More details are given in Section~\ref{ss:comparison}.



\section{The CLuBS Translation Proposal}
\label{s:proposal}

When the proposal for the project was written, the state of the art for machine translation was Statistical Machine Translation systems (SMT). SMT was envisaged as the main architecture for translating parts of the metadata (titles and abstracts). For translating queries and keywords, multilingual thesauri and controlled vocabularies were chosen as the source input is not made by complete sentences.

During the course of the project, we have entered the boom of deep learning, especially for MT. Other fields benefited from deep learning before, but for MT, Neural Machine Translation (NMT) became state-of-the-art in 2016. Taking place the project in the middle of this transition, we have decided to follow the two approaches and chose which architecture to integrate after its evaluation on the retrieval performace. Notice that the translation quality does not need to be related to the final retrieval quality, and a translation system with a high adequacy could be better for retrieval than a system with a very high fluency, which is usually preferred by humans.

Section~\ref{ss:comparison} summarises the main differences between translation architectures for titles and abstracts. For queries and keywords, we sketch the translation proposal  via mapping approaches in deliverable M1.5.

%  \definecolor{greenp}{rgb}{0.01, 0.75, 0.24}
%  \definecolor{crimson}{rgb}{1.0, 0.0, 0.25}
 \definecolor{greenp}{rgb}{0, 0, 0.24}
 \definecolor{crimson}{rgb}{0, 0, 0.25}
\begin{table}[t]
\begin{center}
 \begin{tabular}{lccc}
  \toprule
                  & {\bf RBMT} & {\bf SMT} & {\bf NMT}\\
   \midrule
  Data Amount     & small & large & large\\
  Training Time   &   --  & {\color{greenp}days} & {\color{crimson}weeks}\\
  CPU/GPU         & CPU   & CPU  & {\color{crimson}GPU}\\
  Cost            & {\color{crimson}expensive}  & {\color{greenp}cheap} & expensive  \\ 
                  & {\color{crimson}(in people)} &  &  (in hardware) \\ 
  Maintainability &  weak & strong  & {\color{greenp}superstrong} \\
 \midrule
  Grammaticality  & {\color{greenp}strong} & medium & {\color{greenp}strong}\\
  Reordering      & strong & {\color{crimson}weak} & strong\\
  Consistency     & {\color{greenp}strong} & medium & {\color{crimson}weak}\\
  Coverage        & {\color{crimson}weak}   & {\color{greenp}strong} & weak\\
  Multilinguality  & medium & none  & {\color{greenp}strong} \\
  \bottomrule
\end{tabular}
\end{center}
 \caption{Comparison of the characteristics of the main kinds of translation engines: rule-based (RMBT), statistical (SMT) and neural (NMT). The top rows show  the characteristics to be taken into account for deployment and the bottom rows the quality achieved for different linguistic issues.}
 \label{tab:comparison} 
\end{table}




\subsection{Neural Machine Translation vs. other Systems}
\label{ss:comparison}

The quality of neural systems is currently superior to other translation systems for language pairs with large amounts of parallel data.
NMT is specially better than SMT in fluency which makes its output more appealing to humans. The decoder side of an NMT system is basically a language model, so, by construction, good fluency is expected. However, NMT shows very annoying TOCHANGE characteristics regarding adequacy. Since word embeddings take care of the alignments and similarities, synonyms are more likely to appear, but not only synonyms, any kind of related word. So, it is not strange that the sentence "I have 72 books in my library." is translated as "Ich habe 79 BÃ¼cher in meiner Bibliothek." as the embedding for 72 and 79 will be very similar. NMT systems also create and delete words at will, there is not real control of the number of words needed as it was done by word and phrase penalties in SMT. One could say that an SMT system performs literal translation, while NMT performs free ---kind of artistic--- translation.

SMT systems allow more control because they follow a pipeline of processes, whereas NMT systems are end-to-end architectures. This feature has pros and cons at the same time. One can improve the output of a module before feeding the next one (e.g. discarding low frequent alignments before phrase extraction or pruning a phrase table before decoding) but also errors in one module are propagated into the others. In standard NMT, error propagation cannot occur but neither can the improvement of specific processes. Even with these problems, NMT systems are nowadays state-of-the-art at least for resource-rich language pairs and trigger new functionalities such as multilinguality and zero-shot translation, that is, translation for language pairs not directly seen in training but included in other pairs. In the CLuBS project, we have in-doimain parallel data for French--English and German--English but not for French--German for instance, and we could approach that language pair as a zero-shot one. It is worth then trying to adapt the basic architecture to tackle its specific drawbacks.

Regarding the best system for deployment, SMT systems are easier and faster to train and all the process can be done in commodity hardware. On the other hand, NMT systems need at least a week of training time using a GPU. In both cases decoding can be done using CPUs with competitive speeds. As said before, an additional advantage of NMT systems for CLuBS is maintainability, since a single system can be used to translate among the 12 translation directions. Adding new data via transfer learning is another advantage of NMT that would allow the improvement of the translator in our domain as PubPsych adds new documents with parallel content.

Table~\ref{tab:comparison} summarises the pros and cons of each kind of architecture. The top rows show  the characteristics to be taken into account for deployment and the bottom rows the quality achieved for different linguistic issues. We have included Rule-based Machine Translation systems (RBMT) for completeness, even if these systems are not adequate for CLuBS due to the languages involved and the amount of time required to develop an in-domain engine.

% The NMT community should also try to help MT consumers in moving from SMT to NMT. There are several functionalities that SMT allowed* not still available in NMT, most of them needing more research in order to be implemented. A non-exhaustive list with its implications follows:
% 
% - Include external knowledge and force translation of a given phrase. This allows to hibridise systems, to include TMs, to systematically translate number and dates, to include parallel controlled vocabularies, etc. 
%    -- need to transfer information from source to target
% - Auto data cleaning. This allows to successfully use crawled (low quality) parallel for training
%    -- need to discard low quality sentence pairs during training (the equivalent is IBM models not considering non-frequent alignments)
% - Reordering constraints. This allows monotonous translations at punctuation for instance
%    -- need to force positions in target?
% - What else??

% (*) "allow" can be translated here as "could be done by Moses used as a black box" 


\subsection{Neural Machine Translation for CLuBS}
\label{ss:system}

There are several aspects of NMT systems that should be studied during the course of the project in order to approach specific necessities of our setting. As NMT is a rapidly evolving field, this implies research that can be divided into two main categories: 

\begin{description}
 \item[Architecture.] We have at our disposal multilingual thesauri such as MeSH for the domain of psychology. But in NMT, adding external knowledge and forcing translation of a given phrase is not possible in standard systems. We will explore how to implement these features. 
%  This allows to hibridise systems for example by including parallel controlled vocabularies to force the translation of in-domain words. 
 Two non-trivial topics are approached here: the addition of external resources during training and the transfer of information from source to target.
% - Auto data cleaning. This allows to successfully use crawled (low quality) parallel for training
%    -- need to discard low quality sentence pairs during training (the equivalent is IBM models not considering non-frequent alignments)
 \item[Data.] High quality in-domain parallel data is indispensable for successfully training a system but we do not have data on psychology for all the needed language pairs. We therefore aim to explore the gathering of this data beforehand, and also how to perform an internal auto-data cleaning during training as this would allow to successfully use crawled (low quality) parallel sentences as appropriate.
\end{description}




\section{Conclusions}
\label{s:conclusions}

Several approaches can be followed to develop a machine translation engine. CLuBS will focus on variants of SMT and NMT engines, whose characteristics best suit our settings. The final prototype will implement the best performing system in our IR evaluation but, during the project, we will implement at least a variant of the two main architectures. Due to the novelty, higher performance and room for improvement, research will be carried out for NMT and this will provide CLuBS with several system variants.

%
% ---- Bibliography ----
%
% \addcontentsline{toc}{section}{References}
% \bibliographystyle{plain}
% \bibliography{genericMT}


\end{document}
